{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408acf92",
   "metadata": {},
   "source": [
    "\n",
    "# üß† An√°lise de Sentimentos - Google Play Store Reviews\n",
    "\n",
    "Este projeto tem como objetivo construir um modelo de Machine Learning para classificar avalia√ß√µes de usu√°rios como **positivas** ou **negativas** com base no conte√∫do textual das reviews na Google Play Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re, string, joblib\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f0bdd",
   "metadata": {},
   "source": [
    "### üîπ Carga dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://raw.githubusercontent.com/xxjelax/playstore-sentiment-analysis/refs/heads/main/dataset/reviews.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9cf07",
   "metadata": {},
   "source": [
    "### üîπ An√°lise Explorat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b01598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "sns.countplot(x='score', data=df)\n",
    "plt.title('Distribui√ß√£o das Avalia√ß√µes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bed05",
   "metadata": {},
   "source": [
    "### üîπ Pr√©-processamento e Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.dropna(subset=['content', 'score'], inplace=True)\n",
    "df = df[df['score'] != 3]\n",
    "df['sentiment'] = df['score'].apply(lambda x: 1 if x >= 4 else 0)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['clean_content'] = df['content'].apply(clean_text)\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e125927",
   "metadata": {},
   "source": [
    "### üîπ Separa√ß√£o Treino/Teste (Holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "X = df['clean_content']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30c57d",
   "metadata": {},
   "source": [
    "### üîπ Etapa 8 ‚Äì Modelagem com Algoritmos Cl√°ssicos (Linha de Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baseline_models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=SEED)\n",
    "}\n",
    "\n",
    "for nome, modelo in baseline_models.items():\n",
    "    print(f\"\\nüìå Modelo (linha base): {nome}\")\n",
    "    pipe = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('model', modelo)\n",
    "    ])\n",
    "    y_pred = cross_val_predict(pipe, X_train, y_train, cv=5)\n",
    "    print(\"Accuracy:\", accuracy_score(y_train, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_train, y_pred, target_names=[\"Negativo\", \"Positivo\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a45301",
   "metadata": {},
   "source": [
    "### üîπ Etapa 9 ‚Äì Otimiza√ß√£o de Hiperpar√¢metros (GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelos_grids = {\n",
    "    'Naive Bayes': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', MultinomialNB())\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'tfidf__max_df': [0.8, 1.0],\n",
    "            'tfidf__min_df': [2, 5],\n",
    "            'model__alpha': [0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', SVC())\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'tfidf__max_df': [0.8, 1.0],\n",
    "            'tfidf__min_df': [2, 5],\n",
    "            'model__C': [1, 10],\n",
    "            'model__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'tfidf__max_df': [0.8, 1.0],\n",
    "            'tfidf__min_df': [2, 5],\n",
    "            'model__n_neighbors': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('model', DecisionTreeClassifier(random_state=SEED))\n",
    "        ]),\n",
    "        'param_grid': {\n",
    "            'tfidf__max_df': [0.8, 1.0],\n",
    "            'tfidf__min_df': [2, 5],\n",
    "            'model__max_depth': [10, 20, None]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "modelos_treinados = {}\n",
    "for nome, config in modelos_grids.items():\n",
    "    print(f\"\\nüîç Treinando modelo com GridSearchCV: {nome}\")\n",
    "    grid = GridSearchCV(config['pipeline'], config['param_grid'], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    modelos_treinados[nome] = grid\n",
    "    print(f\"Melhores par√¢metros: {grid.best_params_}\")\n",
    "    print(f\"Melhor score de valida√ß√£o cruzada: {grid.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db44a72",
   "metadata": {},
   "source": [
    "### üîπ Etapa 10 ‚Äì Avalia√ß√£o dos Modelos Otimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultados = {}\n",
    "for nome, modelo in modelos_treinados.items():\n",
    "    print(f\"\\nüìå Avalia√ß√£o Final - {nome}\")\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Negativo\", \"Positivo\"]))\n",
    "    resultados[nome] = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5649a52",
   "metadata": {},
   "source": [
    "### üîπ Etapa 11 ‚Äì Exporta√ß√£o do Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "melhor_nome = max(resultados, key=resultados.get)\n",
    "melhor_modelo = modelos_treinados[melhor_nome]\n",
    "joblib.dump(melhor_modelo, 'sentiment_model.pkl')\n",
    "print(f'Modelo salvo: {melhor_nome}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433806c6",
   "metadata": {},
   "source": [
    "\n",
    "### ‚úÖ Etapa 12 ‚Äì Conclus√µes\n",
    "\n",
    "O modelo com melhor desempenho foi selecionado ap√≥s an√°lise de diversos algoritmos usando valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros. As m√©tricas de precis√£o, recall e F1-score indicam que o modelo est√° apto a identificar avalia√ß√µes positivas e negativas com alta confiabilidade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf029eb3",
   "metadata": {},
   "source": [
    "\n",
    "### üîó Etapa 13 ‚Äì Refer√™ncias\n",
    "\n",
    "- Dataset: [Kaggle - Google Play Store Reviews](https://www.kaggle.com/datasets/prakharrathi25/google-play-store-reviews)\n",
    "- Scikit-Learn documentation: https://scikit-learn.org/\n",
    "- NLTK Stopwords: https://www.nltk.org/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
